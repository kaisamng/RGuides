# Confidence Intervals

::: {style="display: grid; grid-template-columns: 6fr 1fr; grid-column-gap: 10px;"}

<div>
Suppose you want to know the true proportion of Freshmen at TJ who've had Boba tea. You select a SRS of 30 students and ask them if they have had Boba tea. You find out that, among your sample, 65% have had Boba tea.

Up to now, the promise in this class that that we'll give you the tools to estimate the true population parameter using only a sample. So now we return to this question: how close were you to the **true** percentage of TJ Freshmen who've ever had Boba tea? 
</div>

<div style="border: none; margin: 0rem;">
```{r, echo=FALSE}
knitr::include_graphics("resources/images/15-One-Prop-CIs/boba.png")
```
</div>

:::

Here's what we know:

- We know that our sample was not biased, because it was an SRS.
- We know that this one sample is part of a sampling distribution, that is approximately normal.
- We know that if we took another sample, our mean sample statistic will be different, because of **sampling variability**.
- **Sampling variability** fundamentally means that we know we can *sort of* trust our sample, but we can't completely trust it-- because we know for a fact that our sample statistic is not *exactly* equal to the true population parameter.




::: {style="display: grid; grid-template-columns: 6fr 1fr; grid-column-gap: 10px;"}

<div>
On top of that, we still don't know how close we are to the sample, because we don't know the population standard deviation. It is completely possible that we got very unlucky, and our sample is completely unrepresentative of the population. The chances that we get an unrepresentative sample with SRS is smaller, but it is not 0. 

If you were to guess the number of jelly beans in a jar, you're more likely to be correct by giving a range of values, rather than a single number. We are more likely to *capture* the "true" population parameter with a *range* of values instead of a single estimate.  This range of values is called a **confidence interval**.
</div>

<div style="border: none; margin: 0rem;">
```{r, echo=FALSE}
knitr::include_graphics("resources/images/15-One-Prop-CIs/jelly_beans.png")
```
</div>

:::



## Interpreting Confidence Intervals

Every confidence interval is constructed by using the **point estimate**. The point estimate is the sample statistic that you calculated initially. 

From there, we create an upper and lower bound by adding and subtracting a **margin of error** from the point estimate. In other words:

$$ \text{Confidence Interval}= \text{Point Estimate} \pm \text{Margin of Error}$$

The margin of error, is based on standard deviation of the sampling distribution (as referenced in \@ref(samp-prop) and \@ref(samp-mean)) and the **critical value** derived from the **Confidence Level**. In other words, $\text{Margin of Error}= \text{critical value} * \text{standard deviation of sampling distribution}$ (You do not need to know the Margin of Error calculation yet, as we will revisit this later.)

Graphically speaking, a confidence interval looks like this:


```{r, echo=FALSE, fig.cap="Confidence Interval, Annotated", out.width="70%"}
knitr::include_graphics("resources/images/15-One-Prop-CIs/CI_Annotation.png")

```

## Interpreting Confidence <u>Levels</u>

Your final grade in this class will be between an F and an A. 

This range guarantees that I am always right about what your final grade in this class is. But it's also useless. 

On the other hand, "your final grade in this class will be between 92.3% and a 92.4%" is incredibly narrow and gives you a lot of info, but also increases the chance that I will be wrong. When we pick a range of values, we need to ensure that it lies in a good middle ground between being too narrow that we don't capture the value, but not too wide that the confidence interval becomes useless.

::: {style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 10px;"}

<div>
A **Confidence Level** determines how "wide" will be in order to properly capture the true population parameter. It is usually 95%, or 99%. The **critical value** is the associated z-score with the middle % of the sampling distribution. 

</div>

<div>
```{r, echo=FALSE, fig.cap="The middle 95% of the Normal distribution is associated with the z-scores 1.96 and -1.96."}
knitr::include_graphics("resources/images/15-One-Prop-CIs/Norm_95_Percent.png")

```

</div>

:::

If you repeatedly took samples from the population of size $n$, you can expect that 95% of the confidence intervals that you construct will capture the population parameter. 

```{r, echo=FALSE, fig.cap="Repeatedly creating multiple confidence intervals over and over. THis is a recording from https://seeing-theory.brown.edu/frequentist-inference/index.html#section2"}
knitr::include_graphics("resources/images/15-One-Prop-CIs/95CI.gif")

```

Notice that, sometimes, we still get an unlucky sample that, even with the wider width, fails to correct capture the true population parameter. That confidence interval is highlighted in red. 

::: {.rmdwarning}
A confidence level of $95%$ does **NOT** mean that the single confidence interval you create has a 95% probability of capturing the true population parameter. 

Your confidence interval either captures the true population parameter, or it doesn't capture it. That probability is either 0% or 100%. 

A 95% confidence interval states that, **over repeated sampling**, around 95% of the confidence intervals you construct will capture the true population parameter.
:::

If you lower the confidence level, the width of the confidence intervals decreases, and you get more red confidence intervals.

```{r, echo=FALSE, fig.cap="Lowering the confidence leve leads to more precise intervals, but increases the likelihood of not correctlly capturing the population parameter."}
knitr::include_graphics("resources/images/15-One-Prop-CIs/35CI.gif")

```






## Confidence Intervals of One Proportion
































































## Confidence Interval of One Mean